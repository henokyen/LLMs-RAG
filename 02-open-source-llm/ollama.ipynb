{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8247dc5-2534-4e24-8a7f-13d3185c1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-20 19:11:13--  https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-06-20 19:11:13 (9.18 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd361119-ea75-427e-a4c9-5b74ed25f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-27 16:23:43--  https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/documents.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 658332 (643K) [text/plain]\n",
      "Saving to: ‘documents.json’\n",
      "\n",
      "documents.json      100%[===================>] 642.90K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-06-27 16:23:43 (17.9 MB/s) - ‘documents.json’ saved [658332/658332]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/01-intro/documents.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f636ca-af01-4348-a263-e18b94c96380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'f00b924e4079', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'GyMuHZySTR63W2tNnHTAxA', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "from elasticsearch import Elasticsearch\n",
    "#start elastic search from docker image first\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7775d8d-f9bd-4a10-aa6f-19f660f6928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url= 'http://localhost:11434/v1/',\n",
    "               api_key=\"ollama\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69ef1ef-9dac-4156-a4cf-38b456467d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json', 'r') as f:\n",
    "    doc_raws = json.load(f)\n",
    "documents = []\n",
    "for course_dict in doc_raws:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d6cd2a-06d0-401e-8fb1-e521699080ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d50bc84b584f3fbddafc2df11ee756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "index_name = \"module_02_index\"\n",
    "es_client.indices.create(index = index_name, body = index_settings)\n",
    "from tqdm.auto import tqdm\n",
    "for doc in tqdm(documents):\n",
    "    es_client.index(index = index_name, document = doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88afb710-0be6-4261-9f0d-5760e6a7e0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f3379569630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = minsearch.Index(text_fields = ['question', 'text', 'section'],\n",
    "                         keyword_fields = ['course']\n",
    "                        )\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992b345f-e6c1-43cd-9366-9f84c6bb4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "    result  = index.search(query = query,\n",
    "                 boost_dict = boost,\n",
    "                filter_dict= {'course':'data-engineering-zoomcamp'},\n",
    "                 num_results = 5\n",
    "                )\n",
    "    return result\n",
    "\n",
    "def elastic_search (query):\n",
    "    search_query = {\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es_result = es_client.search(index = index_name, body=search_query)\n",
    "    es_result_docs = []\n",
    "    for hit in es_result['hits']['hits']:\n",
    "        es_result_docs.append(hit['_source'])\n",
    "\n",
    "\n",
    "    return es_result_docs\n",
    "    \n",
    "def build_prompt(query, search_result):\n",
    "\n",
    "    context = \"\"\n",
    "    for doc in search_result:\n",
    "        context =context + f\"section :{doc['section']} \\nquestion:{doc['question']}\\nanswer:{doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    You are a course teaching assistant. Answer the given QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "    \n",
    "    QUESTION: {question}\\n\n",
    "    CONTEXT: {context}\n",
    "    \"\"\".strip()\n",
    "    prompt = prompt_template.format(question = query, \n",
    "                                    context = context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm (prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model= 'phi3',\n",
    "        messages = [{'role':'user', 'content':prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0df430f-eb3b-4ade-ac58-34e0e2a2c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    results = search(query)#search(query)\n",
    "    prompt = build_prompt(query, results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "869badb2-cc2c-4c36-8cb9-8a99befaddc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' As a course teaching assistant, there\\'s no direct context provided on how to stop Ollama based on the given FAQ sections. However, considering general practices:\\n\\nTo ensure that systems like Ollama (a hosted instance of Rust-based machine learning and AI environment) are stopped or decommissioned properly when they’re no longer needed in a shared computing space such as cloud instances, here is an answer you could follow. Remember to replace this with specific instructions for your course setup if available:\\n\\n\"To stop Ollama (or similar services), first ensure that any tasks and processes using it are complete or have been saved externally without dependencies on the service itself. Then consult the official documentation, FAQs, or reach out directly via support channels provided by whoever runs your course\\'s server environment to find instructions tailored for stopping Ollama safely.\"\\n\\nIf this question pertains specifically to how students should manage resources in a cloud instance such as Google Cloud Platform (GCP), the answer from FAQ QUESTION 5 would not apply, since it mentions deleting an entire VM rather than managing space. Instead:\\n\\n\"When you run out of storage on your GCP virtual machine due to large datasets or files during ETL processes in Ollama environments, consider purging unnecessary data and setting up proper file handling practices as suggested by QUESTION 2 from the General course-related FAQs. Furthermore, be mindful that it\\'s often not recommended to manually delete VM instances for a learning environment unless explicitly directed due to potential impact on shared resources or dependencies.\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how do i stop ollama?\"\n",
    "rag(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c199d8-c63c-4bf4-863b-81defe22950b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
